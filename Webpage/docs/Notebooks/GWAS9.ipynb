{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a0c39d8",
   "metadata": {
    "Rmd_chunk_options": "setup, include=FALSE",
    "jupyter": {
     "output_hidden": true,
     "source_hidden": true
    },
    "kernel": "R",
    "tags": [
     "scratch"
    ]
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = TRUE)\n",
    "set.seed(19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aad421c",
   "metadata": {},
   "source": [
    "This document is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).\n",
    "\n",
    "The slide set referred to in this document is \"GWAS 9\".\n",
    "\n",
    "\n",
    "Consider GWAS data on $n$ individuals and $p$ SNPs.\n",
    "**GWAS summary statistics** can include a varying combination of the following information,\n",
    "for each variant $l$, or region $\\textrm{reg},$\n",
    "\n",
    "* **Association statistics**\n",
    "$A_l = \\left(\\text{EA}_l, \\widehat{\\beta}_{l}, \\textrm{SE}_{l}, P_{l} \\right)$ where\n",
    "EA is the effect allele for which $\\widehat{\\beta}_{l}$ is reported.\n",
    "These statistics are of size $4p$, and hence their proportion compared to the\n",
    "whole data is $4/n$.\n",
    "\n",
    "* **Information statistics** $I_l = (\\textrm{EAF}_l, \\textrm{INFO}_l, \\textrm{QC}_l,\\ldots),$\n",
    "where EAF is the effect allele frequency (sometimes given only in controls),\n",
    "INFO is an imputation information score, and QC includes\n",
    "quality control measures, such as Hardy-Weinberg P-value and missingness rate of the\n",
    "genotype calls. Proportion to raw data is around $10/n$ (assuming 10 pieces of information\n",
    "per variant).\n",
    "\n",
    "* **LD-matrix** $R_{\\textrm{reg}}$ for certain region(s) of the genome.\n",
    "For a region of size $p_{\\text{reg}}$, the size of $R_{\\textrm{reg}}$\n",
    "is about $\\frac{1}{2}p_{\\text{reg}}^2$, whereas that of the raw data is\n",
    "$n p_{\\text{reg}}$.\n",
    "Thus, the ratio is $p_{\\text{reg}}/(2n)$.\n",
    "\n",
    "When $n$ is of order 1e+5, the summary statistics take only a tiny fraction\n",
    "of the space required by the raw data. Additionally, raw genotype-phenotye\n",
    "data are sensitive, personal data and cannot be shared freely,\n",
    "whereas usually there is no legal restrictions on sharing the GWAS summary statistics.\n",
    "For these reasons, large consortia, such as [GIANT](https://portals.broadinstitute.org/collaboration/giant/index.php/GIANT_consortium) or [CARDIoGRAMplusC4D](http://www.cardiogramplusc4d.org/data-downloads/), are distributing their results as summary statistics. Therefore there is a need for methods that can\n",
    "further analyze the summary statistics, e.g., in fine-mapping, in imputation,\n",
    "in heritability estimation or in gene-level testing.\n",
    "The utilization of summary statistics is\n",
    "reviewed by [Pasaniuc & Price 2017](https://www.nature.com/articles/nrg.2016.142).\n",
    "\n",
    "**Example 9.1.** Reminder how the central association statistics\n",
    "are related to each other.\n",
    "\n",
    "* If we are given `b.est=`$\\widehat{\\beta}$ and `se=`$\\textrm{SE}$, we can compute\n",
    "the P-value as\n",
    "`pchisq((b.est/se)^2, df=1, lower=F)`.\n",
    "\n",
    "* If we are given $\\widehat{\\beta}$ and P-value `pval`, we can compute SE as\n",
    "`sqrt(b.est^2 / qchisq(pval, df=1, lower=F))`.\n",
    "\n",
    "* If we are given SE and P-value, we can compute `b.est=`$\\widehat{\\beta}$\n",
    "for the trait increasing allele\n",
    "as `sqrt(se^2*qchisq(pval, df=1, lower=F))`.\n",
    "(We need to know which allele is the trait increasing from external information.)\n",
    "\n",
    "* Assuming that no (strong) covariates have been applied,\n",
    "we can further infer one of sample size, MAF or case-proportion\n",
    "from SE given that the other two are known, according to the formulas in GWAS3.\n",
    "We can also estimate an SE from these study parameters, and use it with a known\n",
    "effect estimate to derive a P-value.\n",
    "\n",
    "In these notes, we go through how the association summary statistics are produced\n",
    "through *meta-analysis* of individual GWAS and how some analyses, that\n",
    "we have so far done from raw data, could be done using only the summary data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcefa349",
   "metadata": {},
   "source": [
    "### 9.1. Meta-analysis\n",
    "\n",
    "Suppose we have results from $K$ independent GWAS on the same phenotype.\n",
    "(Here independent means that the samples of the GWAS are not overlapping.)\n",
    "Hence,\n",
    "for each variant $l$, we have $K$ sets of GWAS association statistics\n",
    "$A_{kl} = \\left(\\widehat{\\beta}_{kl}, \\textrm{SE}_{kl}, P_{kl} \\right).$\n",
    "How do we combine these $K$ pieces of information\n",
    "into a single combined estimate of the effect size, SE and P-value?\n",
    "\n",
    "A combination of summary-level results from two or more studies into a single\n",
    "estimate is called **meta-analysis**\n",
    "(\"meta\" refers to something happening at a higher level, meta-analysis is an \"analysis of analyses\"). A review of meta-analysis in GWAS\n",
    "by [Evangelou & Ioannidis 2013](https://www.nature.com/articles/nrg3472).\n",
    "\n",
    "In practice, all large GWAS are nowadays meta-analyses carried out by\n",
    "international consortia and a consortium may contain even over\n",
    "hundred individual studies.\n",
    "Often each study runs a GWAS and shares the summary statistics with a\n",
    "centralized analysis group that carries out the meta-analysis.\n",
    "While this approach avoids sharing sensitive individual-level genotype-phenotype data\n",
    "and also operates only with the light-weight summary data,\n",
    "it also restricts the set of possible downstream analyses,\n",
    "since only the marginal additive effect estimates are available.\n",
    "It has become clear that in order\n",
    "to maximize the scientific output from the consortia efforts,\n",
    "future meta-analyses should be designed so that all raw data\n",
    "will be collected in a single place.\n",
    "Unfortunately, this is not always easy because of legal issues related to\n",
    "the sharing of genotype-phenotype data.\n",
    "\n",
    "\n",
    "Let's get back to the question how\n",
    "do we combine $K$ sets of\n",
    "GWAS association statistics on same (or at least similar) phenotype:\n",
    "$A_{kl} = \\left(\\widehat{\\beta}_{kl}, \\textrm{SE}_{kl}, P_{kl} \\right)$.\n",
    "\n",
    "The answer depends on what we assume about the possible variation between\n",
    "the true underlying effects $\\beta_{kl}$ for $k=1,\\ldots,K$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393faaf4",
   "metadata": {},
   "source": [
    "#### 9.1.1 Fixed-effects model\n",
    "\n",
    "The most common assumption is that all studies are measuring the same underlying\n",
    "quantity, i.e., $\\beta_{1l}=\\ldots = \\beta_{Kl}$ and there are no (noticeable)\n",
    "differences in phenotype definitions and no distinct biases between the studies.\n",
    "This is called the fixed-effects model because the effect size is the same across the studies.\n",
    "In this case, the statistically most efficient unbiased linear estimator of the\n",
    "common effect size $\\beta$ is the **inverse-variance weighted (IVW)** estimator\n",
    "(here denoted by F, Fixed-effect estimator):\n",
    "\n",
    "\\begin{align}\n",
    "\\widehat{\\beta}_{l,F} &= \\frac{w_{1l}\\widehat{\\beta}_{1l} + \\ldots + w_{Kl}\\widehat{\\beta}_{Kl}}{w_{1l}+\\ldots +w_{Kl}}\\\\\n",
    "\\text{SE}_{l,F} &= \\left(w_{1l} + \\ldots + w_{Kl} \\right)^{-\\frac{1}{2}},\\quad \\text{where the weight}\\\\\n",
    "w_{kl}&= \\frac{1}{\\text{SE}_{kl}^2} \\text{ is the inverse-variance of study } k.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "In statistics, the inverse of the variance is called **precision**.\n",
    "With that in mind, the above formulas are easy to remember:\n",
    "\n",
    "* The weight given to each study in IVW estimator is proportional to the precision\n",
    "of the study and the weights sum to 1.\n",
    "\n",
    "* The precision of the IVW estimator is the sum of the precisions of the individual studies.\n",
    "\n",
    "\n",
    "**Efficiency of IVW.** It sounds intuitively reasonable to weight each estimate\n",
    "by its precision, but what is the mathematical argument behind this?\n",
    "Let's consider the case of two studies and assume that both\n",
    "yield unbiased estimates of the common effect size $\\beta$, with precisions $w_i=1/\\text{SE}_i^2$\n",
    "for $i=1,2$. This means that\n",
    "$$\\text{E}\\left(\\widehat{\\beta}_i\\, |\\, \\beta\\right) = \\beta \\quad\\text{ and }\\quad\n",
    "\\text{Var}(\\widehat{\\beta}_i) = \\text{SE}_i^2 = \\frac{1}{w_i}, \\quad \\text{ for } i=1,2.\n",
    "$$\n",
    "\n",
    "Consider all possible linear estimators $t(u) = u\\widehat{\\beta}_1 + (1-u)\\widehat{\\beta}_2$\n",
    "determined by value of $u \\in[0,1]$.\n",
    "Estimator $t(u)$ is unbiased for all $u$ as\n",
    "$$\\text{E}(t(u)\\,|\\,\\beta) = u\\text{E}\\left(\\widehat{\\beta}_1\\,|\\,\\beta\\right) + (1-u)\\text{E}\\left(\\widehat{\\beta}_2\\,|\\,\\beta\\right) = u\\beta + (1-u)\\beta = \\beta.$$\n",
    "Thus, on average, any weighting scheme gives the correct answer, and\n",
    "the question is, which one of these weightings gives the most precise\n",
    "combined estimate (= smallest variance).\n",
    "$$\\text{Var}(t(u)) = \\text{Var}\\left(u\\widehat{\\beta}_1\\right) + \\text{Var}\\left((1-u)\\widehat{\\beta}_2\\right)\n",
    "= u^2 \\frac{1}{w_1} + (1-u)^2 \\frac{1}{w_2} = u^2\\left(\\frac{1}{w_1} + \\frac{1}{w_2} \\right)\n",
    "-\\frac{2}{w_2}u+\\frac{1}{w_2}.\n",
    "$$\n",
    "This is a second order polynomial with respect to $u$ and has its minimum where the derivative is 0, i.e.,\n",
    "at $u_0 = \\frac{2/w_2}{2(1/w_1 + 1/w_2)} = \\frac{w_1}{w_1 + w_2},$ which is the IVW.\n",
    "We conclude that IVW is **the minimum variance unbiased linear estimator** of the\n",
    "fixed effect model.\n",
    "\n",
    "\n",
    "\n",
    "**Example 9.2.** Suppose that we do a fixed-effect meta-analysis using\n",
    "IVW of two studies on LDL-cholesterol\n",
    "where the sample sizes of the studies are $n_1=5,000$ and $n_2 =10,000$.\n",
    "If both studies have applied similar covariates and hence have similar\n",
    "error variance $\\sigma_\\varepsilon^2$, then the\n",
    "precisions of the studies are $w_i = 2n_if_i(1-f_i)/\\sigma^2_\\varepsilon$ for $i=1,2.$\n",
    "At a SNP\n",
    "that has same MAF $f$ in both studies, the precisions are proportional to $n_i$\n",
    "and hence the weights of the IVW are $\\frac{w_1}{w_1+w_2}=0.333$\n",
    "and $\\frac{w_2}{w_1+w_2}=0.666$ and the\n",
    "precision of the IVW is $w_F = w_1 + w_2 = 2(n_1+n_2)f(1-f)/\\sigma^2_\\varepsilon$,\n",
    "which is the same as precision from a study with $n_1+n_2=15,000$ samples.\n",
    "Indeed, with linear model, the precision from splitting the data into\n",
    "any subsets and then combining them with IVW is (approximately) the same as\n",
    "doing a joint analysis of all the data with separate intercept terms for each\n",
    "subset (Exercise). (If subsets are small and there are\n",
    "covariates involved, then random noise causes some numerical differences\n",
    "between the approaches.)\n",
    "\n",
    "**Example 9.3.** Suppose that we do IVW meta-analysis of two studies on\n",
    "Parkinson's disease where $n_1=10,000$ of which $3,000$ are cases ($\\phi_1=0.3$)\n",
    "and $n_2 = 6,000$ of which $3,000$ are cases ($\\phi_2=0.5$).\n",
    "Thus the effective sample sizes are $n_{e1}=10000\\cdot 0.3\\cdot 0.7=2100$\n",
    "and $n_{e2}=6000\\cdot 0.5\\cdot 0.5=1500.$\n",
    "If we assume that the MAF of the SNP is the same in both studies,\n",
    "then the precisions of the studies are $w_i = 2n_{ei}f(1-f)$ for $i=1,2$\n",
    "and the weights of the IVW are $\\frac{w_1}{w_1+w_2}=\\frac{2100}{3600}=0.583$\n",
    "and $\\frac{w_2}{w_1+w_2}=\\frac{1500}{3600}=0.417$\n",
    "and the precision of the IVW estimator is $w_F = w_1 + w_2 = 2(n_{e1}+n_{e2})f(1-f),$\n",
    "which is the same as precision from a study with effective sample size of $n_{e1}+n_{e2}$.\n",
    "It can be shown that by splitting the case-control data into subsets, the\n",
    "sum of the effective sample sizes over the subsets is always $\\le$ the effective\n",
    "sample size of the whole data (and the equality holds when the case proportion is constant\n",
    "across the subsets). This gives a technical explanation why,\n",
    "in logistic regression, an inclusion of a\n",
    "binary covariate, such as sex or population label, causes a decrease in precision, and hence increase in SE compared to a single joint analysis of all data without the covariate.\n",
    "This follows because a use of a binary covariate is approximately equivalent to splitting\n",
    "the data by the covariate value, analyzing subsets separately and\n",
    "combining the results using IVW (Exercise).\n",
    "\n",
    "\n",
    "**Example 9.4.** Consider the association statistics at SNP rs11984041 on the large vessel\n",
    "subtype of Ischemic Stroke in\n",
    "[Bellenguez et al. (2012)](https://www.nature.com/articles/ng.1081).\n",
    "They reported a discovery OR 1.50 (1.25-1.79) and replication1 OR\n",
    "1.38 (1.17-1.63) and replication2 OR 1.39 (1.15-1.68), all for the same allele.\n",
    "What is the combined estimate, SE\n",
    "and P-value using the fixed-effects meta-analysis?\n",
    "(They report 1.42 (1.28-1.57), P=1.87e-11.)\n",
    "\n",
    "**Answer.**\n",
    "Let's make a function `meta.F()` that does the\n",
    "IVW meta-analysis for given estimates and SEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aecbdf34",
   "metadata": {
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "meta.F <- function(b.est, se){\n",
    "  #returns inverse-variance weighted meta-analysis estimate, SE and P-value.\n",
    "  b.F = sum(b.est / se^2) / sum(1 / se^2)\n",
    "  se.F = 1 / sqrt(sum(1 / se^2))\n",
    "  p.F = pchisq( (b.F / se.F)^2, df = 1, lower = F)\n",
    "  return(list(b.F = b.F, se.F = se.F, p.F = p.F))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7a962",
   "metadata": {},
   "source": [
    "With these data, we need to compute the SEs from the 95%CIs and then use IVW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f4aa1c",
   "metadata": {
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "b.est = log(c(1.50, 1.38, 1.39)) #beta is logOR for case-control data\n",
    "ci = log(matrix(c(1.25, 1.79,\n",
    "                  1.17, 1.63,\n",
    "                  1.15, 1.68), byrow = T, ncol = 2))\n",
    "se = (ci[,2] - ci[,1])/(2*1.96) #length of 95%CI is 2*1.96*SE\n",
    "meta.res = meta.F(b.est, se)\n",
    "meta.res\n",
    "cbind(OR = exp(meta.res$b.F), low = exp(meta.res$b.F - 1.96*meta.res$se.F),\n",
    "      up = exp(meta.res$b.F + 1.96*meta.res$se.F), pval = meta.res$p.F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ba6fe6",
   "metadata": {},
   "source": [
    "The results match well given the accuracy of two decimals in CIs to compute SE.\n",
    "Let's visualize them by a forest plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b460e9",
   "metadata": {
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "forest.plot <- function(x, intervals, labels = NULL, main = NULL, xlab = \"Effect size\",\n",
    "                      pchs = rep(19,length(x)), cols = rep(\"black\", length(x)),\n",
    "                      cexs = rep(1,length(x))){\n",
    "  K = length(x)\n",
    "  stopifnot(nrow(intervals) == K)\n",
    "  plot(0, col=\"white\", xlim = c( min(c(intervals[,1],0) - 0.05), max(c(intervals[,2],0) + 0.05)),\n",
    "       ylim = c(0, K+1), xlab = xlab, ylab = \"\", yaxt = \"n\",main = main)\n",
    "  axis(2, at = K:1, labels = labels, cex.axis = 0.8)\n",
    "  arrows(intervals[,1], K:1, intervals[,2], K:1,\n",
    "         code = 3, angle = 90, length = 0.02, col = cols)\n",
    "  points(x, K:1, pch = pchs, cex = cexs, col = cols)\n",
    "  abline(v = 0,lty = 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5077599",
   "metadata": {
    "Rmd_chunk_options": "fig.width=6",
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "b.est = c(b.est, meta.res$b.F)\n",
    "ci = rbind(ci, c(meta.res$b.F + c(-1,1)*1.96*meta.res$se.F))\n",
    "labs = c(\"Discv\", \"Rep1\", \"Rep2\", \"Meta\")\n",
    "main.txt = \"rs11984041 Stroke/LVD\"\n",
    "forest.plot(b.est, ci, labels = labs, main = main.txt, xlab = \"logOR\",\n",
    "            pchs = c(19, 19, 19, 18), cexs = c(.8, .8, .8, 1.3), cols = c(1, 1, 1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f27b40",
   "metadata": {},
   "source": [
    "The plot shows that the estimates are very consistent with each other.\n",
    "We also see how the uncertainty has decreased in the combined estimate\n",
    "compared to the individual studies.\n",
    "(In practice, visualizations are recommended to\n",
    "be done with existing R-packages such as `meta` that have many more\n",
    "options.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0d5fad",
   "metadata": {},
   "source": [
    "#### 9.1.2 Heterogeneity\n",
    "\n",
    "When we talk about *heterogeneity* in meta-analysis we mean that\n",
    "the true effect sizes between studies are different.\n",
    "How can we assess that from the observed data?\n",
    "Let's first list heterogeneity measures that are typically reported in meta-analyses.\n",
    "\n",
    "**Cochran's Q.** Assume as the null hypothesis that all $K$\n",
    "studies are measuring the same effect $\\beta$ and\n",
    "that the errors are Normally distributed:\n",
    "$\\widehat{\\beta}_k \\sim \\mathcal{N}(\\beta,\\text{SE}_k^2)$, for study $k$.\n",
    "Then each $z_k = (\\widehat{\\beta}_k - \\beta)/\\text{SE}_k \\sim \\mathcal{N}(0,1)$,\n",
    "and, assuming that the studies are independent, the sum of the squares of\n",
    "these $K$ independent standard Normals has a chi-square distribution with\n",
    "$K$ degrees of freedom:\n",
    "$\\sum_{k=1}^K z_k^2 \\sim \\chi_K^2$.\n",
    "When we replace true $\\beta$ with the fixed-effect estimate $\\widehat{\\beta}_F$\n",
    "from IVW method, then we have a heterogeneity measure called **Cochran's Q**:\n",
    "$$Q = \\sum_{k=1}^K\\left(\\frac{\\widehat{\\beta}_k - \\widehat{\\beta}_F}{\\text{SE}_k}\\right)^2\n",
    "=\\sum_{k=1}^K w_k \\left(\\widehat{\\beta}_k - \\widehat{\\beta}_F\\right)^2,\n",
    "$$\n",
    "\n",
    "that under the null hypothesis of the fixed-effect assumption has distribution $\\chi_{K-1}^2$\n",
    "(where one degree of freedom is lost as we have used the data to estimate the common mean $\\widehat{\\beta}_F$ to replace the true $\\beta$).\n",
    "This distribution can be used to derive P-values for heterogeneity.\n",
    "However,\n",
    "when $K$ is small (say < 5), there is little power to see any heterogeneity with this test,\n",
    "and when $K$ is large (say > 100), then even a small heterogeneity becomes statistically\n",
    "significant.\n",
    "\n",
    "**$I^2$.** To change the focus from P-value to the amount of heterogeneity,\n",
    "a heterogeneity index $I^2$ has been [proposed](https://www.bmj.com/content/327/7414/557):\n",
    "$$I^2 = \\frac{Q-(K-1)}{Q} = 1 - \\frac{K-1}{Q}.$$\n",
    "This value is between 0 and 1 and often reported as percentage\n",
    "(negative values are rounded up to 0).\n",
    "The idea is that if $Q$ shows only the null hypothesis expectation of variation\n",
    "(which is $K-1$),\n",
    "then $I^2=0\\%$ indicating no heterogeneity, whereas values > 50% are interpreted as\n",
    "moderate amount of heterogeneity and > 75% high heterogeneity.\n",
    "With a small number of studies, the uncertainty around estimate of $I^2$ is large\n",
    "and little can inferred statistically.\n",
    "\n",
    "**Between study variance $T^2$.**\n",
    "Often a model for heterogeneity between the true effects is a two-stage\n",
    "hierarchical model where the heterogeneity is defined by a variance parameter $T^2$.\n",
    "We assume that first each true effect is sampled from\n",
    "$(\\beta_k|\\beta, T^2) \\sim \\mathcal{N}(\\beta,T^2)$\n",
    "and then our estimates are formed by adding some noise around these values as\n",
    "$(\\widehat{\\beta}_k | \\beta_k) \\sim \\mathcal{N}(\\beta_k, \\text{SE}_k^2).$\n",
    "From this model, a commonly-used estimate for $T^2$ is\n",
    "$$\\widehat{T^2} = \\frac{Q-(K-1)}{\\sum_{k=1}^K w_k - \\frac{\\sum_{k=1}^K w_k^2}{\\sum_{k=1}^K w_k}}.$$\n",
    "\n",
    "This is on the same scale as the (squared) effect sizes, which makes it different\n",
    "from $Q$ and $I^2$ that are independent of the effect size scale.\n",
    "\n",
    "When $T^2>0$ in the model formulation above, we have defined the standard\n",
    "**random-effects** meta-analysis model where the effect sizes across studies\n",
    "are not assumed to be exactly the same but still they are possibly quite similar\n",
    "(namely, when $T$ is small compared to the common mean $\\beta$ of all effects).\n",
    "In statistics literature, it is common to derive a P-value assuming $\\beta=0$ from such a model\n",
    "and call that the random-effects model's P-value.\n",
    "Such a test is not suitable for GWAS\n",
    "where the relevant null hypothesis is that all effects are 0, rather than that\n",
    "only their mean is 0. This issue is explained by [Han & Eskin 2011](https://www.sciencedirect.com/science/article/pii/S0002929711001558)\n",
    "and they also propose a modification that tests the null hypothesis of exactly\n",
    "0 effect in every study.\n",
    "\n",
    "As a conclusion, the three quantities listed above to measure heterogeneity\n",
    "are often reported in meta-analyses, but\n",
    "they are often not that informative in cases where there are only a handful\n",
    "of studies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705cb7e4",
   "metadata": {},
   "source": [
    "#### 9.1.3 Bayesian meta-analysis\n",
    "\n",
    "The question of heterogeneity between studies can be more flexibly defined\n",
    "in the Bayesian framework where a set of models with different assumptions\n",
    "about heterogeneity can be directly compared against each other\n",
    "and the interpretation\n",
    "of the results of the model comparison does not depend on the sample size.\n",
    "\n",
    "Let's remind ourselves\n",
    "how, in GWAS 4, we compared the model with a non-zero effect to the null model\n",
    "using the approximate Bayes factor, ABF.\n",
    "\n",
    "We assumed that under the alternative hypothesis $H_1$, there was a non-zero\n",
    "effect $\\beta \\sim \\mathcal{N}(0,\\tau^2)$\n",
    "whereas under the null hypothesis $\\beta=0$.\n",
    "Then we derived the marginal likelihoods\n",
    "that these models give for the observed data, and these marginal likelihoods\n",
    "were proportional to Normal densities:\n",
    "\n",
    "\\begin{align}\n",
    "P(\\text{Data}\\,|\\,H_1) &= c\\cdot \\mathcal{N}(\\widehat{\\beta};0, \\text{SE}^2 + \\tau^2)\\\\\n",
    "P(\\text{Data}\\,|\\,H_0) &= c\\cdot \\mathcal{N}(\\widehat{\\beta};0, \\text{SE}^2)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "From this we got an approximate Bayes factor in favor of $H_1$ vs. $H_0$ as\n",
    "$$\n",
    "\\text{ABF}_{1:0} = \\frac{P(\\text{Data}\\,|\\,H_1)}{P(\\text{Data}\\,|\\,H_0)}\n",
    "= \\frac{\\mathcal{N}(\\widehat{\\beta};0, \\text{SE}^2 + \\tau^2)}{\\mathcal{N}(\\widehat{\\beta};0, \\text{SE}^2 )}.\n",
    "$$\n",
    "\n",
    "Finally, we derived the posterior probability of $H_1$ under the assumption that\n",
    "one of $H_0$ and $H_1$ is true and that the prior probability of it being\n",
    "$H_1$ was $p_1 = 1-p_0$:\n",
    "$$\n",
    "P(H_1\\,|\\, \\text{Data}) = \\frac{p_1\\cdot \\text{ABF}_{1:0}}{p_0+p_1 \\cdot \\text{ABF}_{1:0}}.\n",
    "$$\n",
    "\n",
    "Thus, in order to get to a probabilistic model comparison,\n",
    "we needed to set values for two parameters:\n",
    "$\\tau$ that determines how large effects\n",
    "we expect to see under $H_1$, and $p_1$ that determines how probable we think that\n",
    "the alternative hypothesis is *a priori*, before we have seen the data.\n",
    "\n",
    "Let's see how we generalize this to multiple studies.\n",
    "We use a multivariate Normal distribution as the prior distribution of the\n",
    "effect size vector\n",
    "$\\pmb{\\beta} = (\\beta_1,\\ldots,\\beta_K)^T \\sim \\mathcal{N}_K(0,\\pmb{\\Theta}),$\n",
    "where the prior matrix $\\pmb{\\Theta}$ is assumed to take the form\n",
    "$$\\pmb{\\Theta} = \\tau^2 \\left[\n",
    "\\begin{array}{cccc}\n",
    "1& \\theta_{12}& \\ldots &\\theta_{1K}\\\\\n",
    "\\theta_{12}& 1& \\ldots &\\theta_{2K}\\\\\n",
    "\\vdots &\\vdots&\\ddots &\\vdots \\\\\n",
    "\\theta_{1K}& \\theta_{2K}&\\ldots& 1\n",
    "\\end{array}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "Thus, the parameter $\\tau^2$ still defines the prior variance of any one effect size\n",
    "parameter, but now effect sizes from two studies may be correlated as defined by\n",
    "prior correlation $\\theta_{ij}$. For example,\n",
    "\n",
    "* the fixed-effect model $H_F$ results if\n",
    "we set all $\\theta_{ij}=1$,\n",
    "\n",
    "* the independent-effect model $H_I$\n",
    "results if we set $\\theta_{ij}=0$,\n",
    "\n",
    "* the  standard random-effect model $H_R(\\rho)$\n",
    "results if we set $\\theta_{ij}=\\rho$ for\n",
    "some value of $\\rho>0,$ where values close to 1 assume only little heterogeneity\n",
    "and values close to 0 assume almost independent effects; our default is $\\rho=0.9$,\n",
    "\n",
    "* the null model $H_0$ is defined by setting $\\tau^2=0$ (and then the values of $\\theta_{ij}$\n",
    "do not matter).\n",
    "\n",
    "The likelihood function defined by the observed data is also proportional\n",
    "to a multivariate Normal density\n",
    "$\\mathcal{N}_K(\\widehat{\\pmb{\\beta}};\\pmb{\\beta}, \\pmb{\\Sigma}).$\n",
    "If we assume that the studies are independent\n",
    "(no overlapping samples), then $\\pmb{\\Sigma}$ is simply a diagonal matrix where\n",
    "the diagonal is $(\\text{SE}_1^2,\\ldots,\\text{SE}_K^2)$.\n",
    "\n",
    "The marginal likelihood for data given the model $m$ (defined by prior variance matrix\n",
    "$\\pmb{\\Theta}_m$) is\n",
    "$$P(\\text{Data} \\, | \\, H_m) = c\\cdot \\mathcal{N}_K(\\widehat{\\pmb{\\beta}};\n",
    "\\pmb{0}, \\pmb{\\Sigma} + \\pmb{\\Theta}_m ).\n",
    "$$\n",
    "\n",
    "Approximate Bayes factor between any two models $m$ and $\\ell$ is\n",
    "$$\\text{ABF}_{m:\\ell} = \\frac{P(\\text{Data} \\, | \\, H_m)}{P(\\text{Data} \\, | \\, H_\\ell) }\n",
    "=\\frac{\\mathcal{N}_K(\\widehat{\\pmb{\\beta}};\n",
    "\\pmb{0}, \\pmb{\\Sigma} + \\pmb{\\Theta}_m )}{\\mathcal{N}_K(\\widehat{\\pmb{\\beta}};\n",
    "\\pmb{0}, \\pmb{\\Sigma} + \\pmb{\\Theta}_\\ell )}.\n",
    "$$\n",
    "\n",
    "If model $m$ is given a prior probability $p_m$ (and $p_0+\\ldots +p_K=1$), then we can\n",
    "compute the posterior probability for model $m$ as\n",
    "$$\n",
    "P(H_m \\, | \\, \\text{Data}) = \\frac{p_m\\cdot \\text{ABF}_{m:0} }{\\sum_{\\ell=0}^K p_\\ell\\cdot \\text{ABF}_{\\ell:0}},\n",
    "$$\n",
    "where we have computed ABFs between all models and the null model\n",
    "(and $\\text{ABF}_{0:0}=1$). Thus, the posterior probability of a model\n",
    "is proportional to the product of the prior probability of the model\n",
    "and ABF of the model.\n",
    "\n",
    "Let's write a function `abf.mv()` that computes ABFs and posterior probabilities\n",
    "for any given set of prior matrices and prior probabilities.\n",
    "First, we need a density for the multivariate normal.\n",
    "(There is also a package `mvtnorm` for that.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "314a5281",
   "metadata": {
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "log.dmvnorm <- function(x, mu = rep(0, length(x)), S = diag(1, length(x)) ){\n",
    "  #returns log of density of MV-Normal(mean = mu, var = S) at x\n",
    "  K = length(mu)\n",
    "  stopifnot(all(dim(S) == K))\n",
    "  stopifnot(length(x) == K)\n",
    "  chol.S = chol(S) #Cholesky decomposition\n",
    "  log.det = 2*sum(log(diag(chol.S))) #log of det(S)\n",
    "  inv.chol.S = solve(t(chol.S)) #inverse of cholesky^T\n",
    "  return(-K/2*log(2*pi) - 0.5*(log.det + crossprod(inv.chol.S %*% (x-mu))))\n",
    "}\n",
    "\n",
    "abf.mv <- function(b.est, Sigmas, prior = rep(1,length(Sigmas))){\n",
    "  #Returns posterior probabilities of the models listed in Sigmas by their\n",
    "  #   total variance matrix (= sum of prior + likelihood variance matrices)\n",
    "  #Returns also ABFs w.r.t the first model in Sigmas.\n",
    "\n",
    "  M = length(Sigmas) #number of models\n",
    "  K = length(b.est) #number of studies\n",
    "  prior = prior/sum(prior)\n",
    "  log.abf = sapply(Sigmas, function(x){log.dmvnorm(b.est, S = x)})\n",
    "  abf = exp(log.abf - log.abf[1]) #abf w.r.t the first model\n",
    "  posterior = prior*abf\n",
    "  posterior = posterior/sum(posterior)\n",
    "  return(list(posterior = posterior, abf = abf))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dea7c3",
   "metadata": {},
   "source": [
    "**Example 9.5.**\n",
    "Let's generate 4 data sets for 10 case-control\n",
    "studies where the effective sample size varies between 250 and 2500 and\n",
    "MAF varies between 0.4 and 0.5.\n",
    "\n",
    "* 1st data set: all studies estimate the same effect $\\beta=0.1$.\n",
    "* 2nd data set: there is heterogeneity and\n",
    "the true effects come from $\\mathcal{N}(0.1, 0.04^2)$.\n",
    "* 3rd data set: there is heterogeneity but no correlation in effects as they come\n",
    "from $\\mathcal{N}(0, 0.1^2).$\n",
    "* 4th data set has a null SNP in all studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ab35b9",
   "metadata": {
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "K = 10\n",
    "n.eff = runif(K, 250, 2500)\n",
    "f = runif(K, 0.4, 0.5)\n",
    "se = 1/sqrt(2*n.eff*f*(1-f)) #SEs\n",
    "w = 1/se^2 #precisions\n",
    "b = 0.1 #true mean of effects in 1 and 2\n",
    "B.est = cbind(rnorm(K, b, se), #fixed effects\n",
    "              rnorm(K, b, sqrt(se^2 + 0.04^2)), #correlated random effects\n",
    "              rnorm(K, rep(0,K), sqrt(se^2 + 0.1^2)), #independent effects\n",
    "              rnorm(K, rep(0,K), se)) #null model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0497a2",
   "metadata": {},
   "source": [
    "Let's then compare 4 models in these data sets:\n",
    "(1) fixed-effects, (2) correlated-effects, (3) independent-effects and (4) the null.\n",
    "\n",
    "We specify these models by their matrices ($\\pmb{\\Sigma} + \\pmb{\\Theta}_m$),\n",
    "run `abf.mv()` and print the forest plot of the data as well as a barplot\n",
    "of the posterior probability across the 4 competing models, assuming the\n",
    "prior probability of each model is the same (= 0.25).\n",
    "\n",
    "Let's also print the standard heterogeneity measures:\n",
    "value of $I^2$ and the P-value from Cochran's $Q$-statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd377df",
   "metadata": {
    "Rmd_chunk_options": "fig.height = 10, fig.width = 6",
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "Sigma = diag(se^2) #this is the variance of likelihood function -- same for all models\n",
    "tau2 = 0.2^2 #prior variance of effect size -- same for all non-null models\n",
    "S.fix = tau2 * matrix(1, K, K) #fixed-effect model has 1s in the correlation matrix\n",
    "S.cor = tau2 * matrix(0.9, K, K) #correlated effects has corr of 0.9 as off-diagonal\n",
    "diag(S.cor) = tau2 #... and corr of 1 on the diagonal\n",
    "S.ind = tau2 * diag(K) #diagonal matrix for independent effects model, off-diagonals = 0\n",
    "S.null = matrix(0, K, K) #null model has 0 effects\n",
    "Var.matrices = list(Sigma + S.null, Sigma + S.fix, Sigma + S.cor, Sigma + S.ind)\n",
    "\n",
    "par(mfrow = c(4,2))\n",
    "for(ii in 1:ncol(B.est)){\n",
    "  #Standard heterogeneity measures:\n",
    "  b.F = sum(w*B.est[,ii]) / sum(w) #IVW estimate under fixed-effect model\n",
    "  Q = sum( w * (B.est[,ii] - b.F)^2 ) #Cochran's Q\n",
    "  pval.Q = pchisq(Q, df = K-1, lower = F)\n",
    "  I2 = 1 - (K-1)/Q #I^2 from Q\n",
    "\n",
    "  #Bayesian model comparison:\n",
    "  abf.out = abf.mv(B.est[,ii], Sigmas = Var.matrices) #by default, prior is uniform\n",
    "  ci = cbind(B.est[,ii] - 1.96*se, B.est[,ii] + 1.96*se) #95%CIs\n",
    "  forest.plot(B.est[,ii], ci, main = paste(\"Data set\",ii), xlab = \"logOR\")\n",
    "  barplot(abf.out$posterior, ylim = c(0,1), cex.sub = 1.3,\n",
    "          sub = paste0(\"I2=\",max(c(0,round(I2*100))),\"%  het P=\",signif(pval.Q,2)),\n",
    "          names.arg = c(\"NULL\", \"FIX\", \"COR\", \"IND\"),\n",
    "          col = c(\"gray\",\"limegreen\",\"orange\",\"dodgerblue\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5428121d",
   "metadata": {},
   "source": [
    "The Bayesian approach gives a way to assess whether there is heterogeneity\n",
    "in the effect sizes by comparing correlated effects model and/or\n",
    "independent effects model to the fixed effect model.\n",
    "Above it indicates heterogeneity in data sets 2 and 3, as expected.\n",
    "Similarly the $I^2$ value together with P-value from $Q$ indicate heterogeneity\n",
    "in sets 2 and 3.\n",
    "\n",
    "Extensions of the Bayesian approach to overlapping samples between studies, e.g., due\n",
    "to shared controls,\n",
    "and to the subset models, where the effect is non-zero only in\n",
    "particular studies, are discussed by [Trochet et al. 2019](https://doi.org/10.1002/gepi.22202).\n",
    "\n",
    "As the final comment about heterogeneity:\n",
    "Whenever you have an interesting variant in a GWAS meta-analysis, make\n",
    "a forest plot over all the cohorts to see how the effects look like\n",
    "and don't rely only on some quantitative heterogeneity measures,\n",
    "especially if there are only a couple of studies included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec1e56",
   "metadata": {},
   "source": [
    "#### 9.1.4 Publication bias\n",
    "\n",
    "A crucial part of all meta-analyses is to include in the analysis *all* the data\n",
    "available on the particular research question. In particular, one should never\n",
    "use the results of the studies to decide which studies to include or leave out\n",
    "since that will obviously bias the results of the meta-analysis.\n",
    "Studies can be left out because of quality issues or differences in phenotypes,\n",
    "for example, but these must be objective criteria that are not based on the\n",
    "results of the study in any one SNP. In general, meta-analyses in epidemiology\n",
    "and social science etc. are hampered by **publication bias**\n",
    "which means that only studies reporting\n",
    "statistically significant results are published whereas null studies never find their\n",
    "way to public. Consequently, a meta-analysis may report a significant effect\n",
    "based on published studies even though there could be another set of unpublished\n",
    "studies that could show that, when all information is combined,\n",
    "there is no effect. The pubication bias\n",
    "is less of a problem in GWAS, because GWAS results are published simultaneously\n",
    "genome-wide, not separately for the \"significant\" SNPs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc0bfb",
   "metadata": {},
   "source": [
    "### 9.2. Further analyses with summary statistics\n",
    "\n",
    "Meta-analysis yields a set of association statistics ($\\widehat{\\beta}$, SE, P-value).\n",
    "Let's look at how we can do some of the downstream analyses with these pieces of\n",
    "information without an access to the full raw genotype-phenotype data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec83664",
   "metadata": {},
   "source": [
    "#### 9.2.1. Joint model of multiple SNPs\n",
    "\n",
    "Let's consider the joint linear model with $p$ SNPs with\n",
    "the mean centered phenotype $y$ and standardized\n",
    "genotypes (and then we can drop the intercept term from the model):\n",
    "$$\\pmb{y} = \\pmb{X^*} \\pmb{\\lambda}^* + \\pmb{\\varepsilon}.$$\n",
    "The least squares estimator and its variance are\n",
    "\n",
    "\\begin{align}\n",
    "\\widehat{\\pmb{\\lambda}}^* &= \\left(\\pmb{X^*}^T\\pmb{X^*}\\right)^{-1} \\pmb{X^*}^T\\pmb{y},\\\\\n",
    "\\text{Var}\\left(\\widehat{\\pmb{\\lambda}}^*\\right) &= \\sigma_J^2 \\left(\\pmb{X^*}^T\\pmb{X^*}\\right)^{-1},\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "* $\\sigma_J^2 = \\text{Var}(\\varepsilon) = \\text{Var}(y) - (\\widehat{\\pmb{\\lambda}}^*)^T \\pmb{R} \\widehat{\\pmb{\\lambda}}^*$ is the error variance from the Joint model and $\\pmb{R}$ is the LD matrix of the SNPs in $\\pmb{X}$.\n",
    "\n",
    "It turns out that these quantities can be written using summary data from the marginal models $\\pmb{y}=\\pmb{x}^*_l\\beta_l+\\pmb{\\varepsilon}_l$, since\n",
    "\\begin{align}\n",
    "\\left(\\pmb{X^*}^T\\pmb{X^*}\\right) &= n \\pmb{R}\\\\\n",
    "\\pmb{X^*}^T\\pmb{y} &= n \\widehat{\\pmb{\\beta}}^*\\\\\n",
    "\\text{Var}(y) &= (\\widehat{\\beta}^*_l)^2 + \\widehat{\\sigma}_l^2 = (\\widehat{\\beta}_l^*)^2 + 2 n f_l(1-f_l)\\cdot \\text{SE}_l^2,\n",
    "\\end{align}\n",
    "where $f_l$ is the MAF of SNP $l$\n",
    "and $\\text{SE}_l$ is the standard error of the *allelic* marginal effect\n",
    "$\\widehat{\\beta}_l$, as reported by GWAS.\n",
    "\n",
    "With these formulas we have that\n",
    "\\begin{align}\n",
    "\\widehat{\\pmb{\\lambda}}^* &= \\pmb{R}^{-1} \\widehat{\\pmb{\\beta}}^*,\\\\\n",
    "\\text{Var}\\left(\\widehat{\\pmb{\\lambda}}^*\\right) &= \\frac{\\widehat{\\sigma_J^2}}{n} \\pmb{R}^{-1},\\\\\n",
    "\\widehat{\\sigma_J^2} &=\n",
    "\\text{median}_{l=1}^p\\left\\{(\\widehat{\\beta}_l^*)^2 + 2 n f_l(1-f_l)\\cdot \\text{SE}_l^2\\right\\} - (\\widehat{\\pmb{\\beta}}^*)^T \\pmb{R}^{-1} \\widehat{\\pmb{\\beta}}^*,\n",
    "\\end{align}\n",
    "where the median is taken over all the available SNPs in $\\pmb{X}$\n",
    "and its function is to reduce noise compared to the corresponding\n",
    "variance estimate taken from any one $l$.\n",
    "In particular, we do not need an access to raw $\\pmb{X}$ and $\\pmb{y}$\n",
    "in order to do a stepwise forward selection or probabilistic fine-mapping\n",
    "as long as we have the marginal GWAS association statistics and the LD matrix available.\n",
    "\n",
    "If association statistics come from an IVW fixed-effect meta-analysis, then the LD-matrix is a\n",
    "weighted sum of the LD-matrices of individual studies, where the weights are proportional\n",
    "to the sample sizes of the studies Before the meta-analysis, one must make sure that\n",
    "all the studies have measured the effects on the same scale in order that the fixed-effect\n",
    "meta-analysis of the effect sizes makes sense. Typically, this is ensured by normalizing the\n",
    "trait to have a variance of 1 in each study before running the GWAS.\n",
    "\n",
    "If the association statistics are coming from a **logistic regression model\n",
    "applied to case-control data**,\n",
    "we modify the above formulas by setting $\\sigma_J^2=1$ and replacing the sample size\n",
    "$n$ with the effective sample size $n_e$.\n",
    "If summary data come from one study, then $n_e = n \\phi(1-\\phi)$,\n",
    "where $\\phi$ is the proportion of cases in data.\n",
    "If summary data are from a meta-analysis over several studies, then $n_e$ is the sum of\n",
    "the effective sample sizes $n_{e}^{(i)} = n^{(i)} \\phi_i (1-\\phi_i)$ over individuals studies,\n",
    "where $n^{(i)}$ is the total sample size (cases + controls) of study $i$ and\n",
    "$\\phi_i$ is the proportion of cases in study $i$.\n",
    "In this meta-analysis case, the LD-matrix is a\n",
    "weighted sum of the LD-matrices of individual studies, where the weights are proportional\n",
    "to the effective sample sizes of the studies.\n",
    "This approximation works well when the effect sizes are not very large,\n",
    "MAF is not very small and $\\phi$ is quite balanced, say, within (0.2,0.8).\n",
    "\n",
    "The idea of computing the joint model from the summary statistics was introduced by\n",
    "[Yang et al. 2012](https://www.nature.com/articles/ng.2213)\n",
    "and has been widely used through the conditional and joint analysis (COJO)\n",
    "module of the software package [GCTA](https://cnsgenomics.com/software/gcta/#Overview).\n",
    "The same idea is used in many fine-mapping software packages such as [FINEMAP](http://www.christianbenner.com/).\n",
    "\n",
    "Let's try it with two SNPs and a quantitative trait. (Uses `geno.2loci()` from Section 7 to generate genotypes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04683a4d",
   "metadata": {
    "Rmd_chunk_options": "echo = F",
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "geno.2loci <- function(n, r, mafs, return.geno = TRUE){\n",
    "  #INPUT:\n",
    "  # n, individuals\n",
    "  # r, correlation coefficient between the alleles on the same haplotype of the two loci\n",
    "  # mafs, MAFs of the two loci\n",
    "  #OUTPUT:\n",
    "  # if return.geno=TRUE: n x 2 matrix of GENOTYPES of n individuals at 2 loci\n",
    "  # if return.geno=FALSE: (2n) x 2 matrix of HAPLOTYPES of n individuals (2n haplotypes) at 2 loci\n",
    "  stopifnot( r >= (-1) & r <= 1 )\n",
    "  stopifnot( length(mafs) == 2 )\n",
    "  stopifnot( all(mafs > 0) & all(mafs <= 0.5) )\n",
    "  stopifnot( n > 0)\n",
    "\n",
    "  #Label SNPs and alleles so that a and b are minor alleles and freq a <= freq b.\n",
    "  #At the end, possibly switch the order of SNPs back to the one given by the user.\n",
    "  f.a = min(mafs) # maf at SNP1\n",
    "  f.b = max(mafs)  # maf at SNP2\n",
    "\n",
    "  #With these parameters, the possible LD coefficient r has values in the interval:\n",
    "  r.min = max( -1, -sqrt(f.a/(1-f.a)*f.b/(1-f.b)) )\n",
    "  r.max = min( 1, sqrt(f.a/(1-f.a)/f.b*(1-f.b)) )\n",
    "  #c(r.min,r.max)\n",
    "  #Check that r is from this interval\n",
    "  if(r < r.min | r > r.max) stop(paste0(\"with these mafs r should be in (\",r.min,\",\",r.max,\")\"))\n",
    "\n",
    "  # Alleles SNP1: A (major) and a (minor); SNP2: B (major) and b (minor).\n",
    "  # Compute conditional probabilities for allele 'a' given allele at locus 2:\n",
    "  q0 = f.a - r*sqrt(f.a*(1-f.a)*f.b/(1-f.b))         #P(a|B)\n",
    "  q1 = f.a + (1-f.b)*r*sqrt(f.a*(1-f.a)/f.b/(1-f.b)) #P(a|b)\n",
    "\n",
    "  #Compute the four haplotype frequencies:\n",
    "  f.ab = f.b*q1\n",
    "  f.aB = (1-f.b)*q0\n",
    "  f.Ab = f.b*(1-q1)\n",
    "  f.AB = (1-f.b)*(1-q0)\n",
    "  f = c(f.ab,f.aB,f.Ab,f.AB)\n",
    "  f #These are the haplotype frequencies in the population.\n",
    "  haps = matrix(c(1,1,1,0,0,1,0,0), nrow = 4, ncol = 2, byrow = T) #4 haplotypes in the population.\n",
    "\n",
    "  #Generate data for n individuals where each individual is measured at these two SNPs:\n",
    "  hap.ind = sample(1:4, size = 2*n, replace = T, prob = f) #There are 2*n haplotypes, 2 for each individual\n",
    "\n",
    "  if(mafs[1] > mafs[2]) haps = haps[,2:1] #Whether to change the order of loci?\n",
    "  #Either make genotype matrix by summing the two haplotypes for each individual...\n",
    "  if(return.geno) X = haps[hap.ind[1:n],] + haps[hap.ind[(n+1):(2*n)],]\n",
    "  if(!return.geno) X = haps[hap.ind,]  #...or return haplotypes as such.\n",
    "  return(X)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a172f34f",
   "metadata": {
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "n = 2000\n",
    "maf = c(0.3, 0.4)\n",
    "lambda = c(0.2, -0.05) #true causal effects\n",
    "r = 0.5 #LD btw SNPs 1 & 2\n",
    "X = geno.2loci(n, r, mafs = maf, return.geno = TRUE)\n",
    "R = cor(X) #LD in data\n",
    "f = colSums(X)/2/n #MAFs in data\n",
    "sc = sqrt(2*f*(1-f)) #scaling constants\n",
    "y = scale(X %*% lambda + rnorm(n, 0, sqrt(1 - var(X %*% lambda))))\n",
    "b.est = rbind(summary(lm(y ~ X[,1]))$coeff[2,1:2],\n",
    "              summary(lm(y ~ X[,2]))$coeff[2,1:2]) #col1 estimate, col2 SE\n",
    "b.s = b.est[,1]*sc #scaled betas\n",
    "l.s.sumstat = solve(R, b.s) #computes scaled lambdas as R^-1 * b.s\n",
    "sigma2.J.sumstat = median(b.s^2 + n*sc^2*b.est[,2]^2) - as.vector(t(b.s) %*% solve(R, b.s))\n",
    "l.s.se.sumstat = sqrt(sigma2.J.sumstat/n * diag(solve(R))) #SEs of scaled lambdas\n",
    "cbind(lambda = l.s.sumstat/sc, se = l.s.se.sumstat/sc) #show on allelic scale\n",
    "#Compare to the joint model on raw data\n",
    "summary(lm(y ~ X))$coeff[2:3,1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a11472",
   "metadata": {},
   "source": [
    "Same results up to the 3rd decimal. (Binary trait case left as an exercise.)\n",
    "\n",
    "Given that the association summary statistics\n",
    "from large meta-analyses are publicly available,\n",
    "it would be very nice if we could do joint models and fine-mapping by\n",
    "combining those statistics with\n",
    "LD-information from some **reference database**,\n",
    "without needing the access to the original genotypes.\n",
    "Indeed, this is how GCTA-COJO analyses are done.\n",
    "Unfortunately, with recent large datasets, such as the UK Biobank,\n",
    "it has become clear that the accuracy of the\n",
    "LD-estimates must increase together with the GWAS sample size.\n",
    "Otherwise, the summary statistic methods start reporting false positives\n",
    "because of the inconsistency between the highly precise effect estimates\n",
    "and the LD information from the reference data\n",
    "([Benner et al. 2017](https://www.sciencedirect.com/science/article/pii/S0002929717303348)).\n",
    "Hence, in general, we will need LD-information from the same data from which the GWAS\n",
    "summary statistics were calculated in order to do reliable fine-mapping and joint analysis.\n",
    "This is one reason why future meta-analyses\n",
    "should be planned in such a way that all data are collected in one place, and why we will\n",
    "need new ways to seamlessly distribute LD-information as another type of GWAS summary statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e964fc7d",
   "metadata": {},
   "source": [
    "#### 9.2.2 Polygenic scores\n",
    "\n",
    "Our goal so far has been to identify **causal variants** that tell about the\n",
    "biology of the phenotype and propose ways for targeted treatments.\n",
    "\n",
    "Another way to utilize GWAS results is to **predict** phenotypes.\n",
    "There is a difference between understanding the causes of a phenomenon\n",
    "and an ability to predict the phenomenon:\n",
    "While understanding typically implies a good prediction,\n",
    "a good prediction does not necessarily require understanding.\n",
    "For example, we do not need to know which of the two variants in high LD\n",
    "with each other is a causal one in order to do a good prediction:\n",
    "Either of the variants will\n",
    "do almost equally well when used in a prediction model,\n",
    "because, due to high LD, they carry almost the same information about\n",
    "the genetic differences between individuals.\n",
    "\n",
    "Let's consider the standard additive model for the phenotype across the whole genome:\n",
    "$$\n",
    "y_i = \\eta_i +  \\varepsilon_i = \\sum_{k=1}^p x_{ik} \\lambda_{k} + \\varepsilon_i.\n",
    "$$\n",
    "If we knew the true causal effects $\\lambda_{k}$, then we could do the perfect prediction\n",
    "of the genetic component $\\eta_i = \\sum_k x_{ik} \\lambda_{k}$ for individual $i$ given her/his\n",
    "genotypes. In the population, this perfect genetic prediction would explain the proportion\n",
    "$h^2$ (=additive heritability) of the total phenotypic variance and this would be as good as\n",
    "an additive genetic prediction ever gets.\n",
    "\n",
    "By a **polygenic score** (PS) we mean an instance of the additive genetic predictor defined\n",
    "by a set of weights $\\pmb{\\alpha} = (\\alpha_k)_{k=1}^p$ that predicts the genetic\n",
    "component of individual $i$ as\n",
    "$$\n",
    "\\text{PS}_i(\\pmb{\\alpha}) = \\sum_{k=1}^p x_{ik} \\alpha_{k}.\n",
    "$$\n",
    "Typically, the weights $\\alpha_k$ are obtained from the GWAS summary statistics $\\widehat{\\beta}_k$,\n",
    "possibly with some variable selection and/or shrinkage of the effect sizes and/or LD-adjustments\n",
    "to approximate the causal effects $\\lambda_k$ rather than the marginal effects $\\beta_k$.\n",
    "\n",
    "Such PS can then be tested against the known phenotype values in a test cohort to see\n",
    "how much phenotypic variation it explains.\n",
    "\n",
    "A recent guide for making PS by [Choi et al. 2020](https://www.nature.com/articles/s41596-020-0353-1)\n",
    "and the corresponding online [tutorial](https://choishingwan.github.io/PRS-Tutorial/).\n",
    "These articles talk about\n",
    "developing and evaluating polygenic risk prediction models\n",
    "([Chatterjee et al. 2016](https://www.nature.com/articles/nrg.2016.27)) and\n",
    "personal and clinical utility of PS\n",
    "([Torkamani et al. 2018](https://www.nature.com/articles/s41576-018-0018-x)).\n",
    "\n",
    "\n",
    "The methods to derive the best possible PS weights $\\alpha_k$ are currently one of the hot\n",
    "topics in the GWAS field since the recent results have shown that the current GWAS\n",
    "summary statistics can already provide useful predictive discrimination for risk of several diseases\n",
    "(slides 17-21). Hope is that by more advanced modeling, the accuracy can be further improved.\n",
    "\n",
    "New methods are typically compared to the two reference methods:\n",
    "**P-value clumping** and [LDpred](https://github.com/bvilhjal/ldpred).\n",
    "Both of these take in the GWAS association statistics and produce PS-weights by accounting for LD.\n",
    "P-value clumping prunes away variants that are\n",
    "in high LD with each other whereas LDpred\n",
    "is a Bayesian method that outputs estimates for the causal effect sizes\n",
    "for each variant by accounting for\n",
    "the LD-structure around the variant.\n",
    "\n",
    "**P-value clumping ($r^2$,$d$,$P_{\\text{thr}}$).**\n",
    "The simplest PS uses the marginal GWAS effect estimates $\\widehat{\\beta}_k$ as weights.\n",
    "Suppose that we have two variants in high LD. Their marginal effects are almost the same\n",
    "and including them both in the PS is likely to\n",
    "overestimate the joint contribution from these two SNPs and, hence, reduce the PS accuracy.\n",
    "To avoid this, we do some **LD-pruning** meaning that we will only include non-zero weights for\n",
    "SNPs that are not in high LD with each other. For example, we may require that $r^2<0.1$ between\n",
    "all pairs of variants that have non-zero weights in PS.\n",
    "In practice, such LD-pruning is applied only within certain window size (e.g., $d=1$Mb)\n",
    "for computational reasons and because\n",
    "LD decays quickly with distance in homogeneous populations.\n",
    "**P-value clumping** means LD-pruning that prefers to leave in the data\n",
    "the variant with the lowest GWAS\n",
    "P-value and prune away its LD-friends that have higher GWAS P-values.\n",
    "This way our final LD-pruned data set contains as many of\n",
    "the top GWAS variants (in terms of the lowest P-values) as possible given the pruning parameter $r^2$.\n",
    "Typically, there is also a P-value threshold (between 0.05 and 5e-8) to ensure that\n",
    "all variants included in PS with non-zero weights will have GWAS P-value $<P_{\\text{thr}}$.\n",
    "\n",
    "**Training, validation and testing.**\n",
    "Let's put the generation of the PS-weights using P-value clumping method to the\n",
    "context of typical prediction model building having three independent data sets for\n",
    "each of training, validation and testing.\n",
    "\n",
    "* Training data is an existing large GWAS on the phenotype of interest\n",
    "where we have access to the marginal association statistics. Ideally,\n",
    "we would also have access to the LD information of the training data,\n",
    "but when this is not possible, we use external reference data from\n",
    "the GWAS population as an approximation to the LD in training data.\n",
    "\n",
    "* Validation data are genotype-phenotype data that we use to tune the parameters\n",
    "of the PS model, namely $r^2$ and $P_{\\text{thr}}$ (while $d$ is often assumed fixed).\n",
    "This means that we will make a set of PS for a grid of values\n",
    "of $r^2$ and $P_{\\text{thr}}$, and test in the validation set how each of them performs.\n",
    "We choose the best performing version of the PS as our final PS.\n",
    "\n",
    "* Testing data are individual level genotype-phenotype data\n",
    "that are independent from training and validation data.\n",
    "Testing data are used only to test the final PS that was chosen at the validation step.\n",
    "The performance of the PS in testing data is expected to generalize to other data sets that have\n",
    "similar properties: same population, same phenotype etc.\n",
    "Note that the performance in validation data may overestimate the performance in\n",
    "some new data since the validation data were used for optimizing the PS parameters.\n",
    "The performance in testing data does not suffer from this problem and\n",
    "it is therefore the final result to report about the PS chosen.\n",
    "\n",
    "[PRSice2](https://choishingwan.github.io/PRSice/) is software to generate and validate PS\n",
    "given the summary statistics and validation data. Also\n",
    "[PLINK2](https://www.cog-genomics.org/plink/2.0/) does P-value clumping and computes PS."
   ]
  }
 ],
 "metadata": {
  "Rmd_chunk_options": {
   "author": "Matti Pirinen, University of Helsinki",
   "date": "Latest update: 2.12.2020; first version: 20-Feb-2019",
   "output": {
    "html_document": "default"
   },
   "title": "GWAS 9: Meta-analysis and summary statistics",
   "urlcolor": "blue"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ],
    [
     "R",
     "ir",
     "",
     ""
    ]
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
