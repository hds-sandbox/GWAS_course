{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d45192",
   "metadata": {
    "Rmd_chunk_options": "setup, include=FALSE",
    "jupyter": {
     "output_hidden": true,
     "source_hidden": true
    },
    "kernel": "R",
    "tags": [
     "scratch"
    ]
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = TRUE)\n",
    "set.seed(19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e25f9cf",
   "metadata": {},
   "source": [
    "This document is licensed under a\n",
    "[Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).\n",
    "\n",
    "The slide set referred to in this document is \"GWAS 4\".\n",
    "\n",
    "The standard statistical inference typically used in GWAS\n",
    "is based on a fixed significance threshold and only determines whether P-value\n",
    "falls below the GWS threshold. Impicitly, however,\n",
    "GWAS make a more efficient use of the observed\n",
    "data than just labelling it significant or not significant\n",
    "as the exact P-values are reported.\n",
    "Intuitively, we feel\n",
    "that if two variants have P-values of 4e-8 and 2e-200,\n",
    "the latter is certain to be a true association,\n",
    "whereas the former might still go away when we collect more data.\n",
    "But we also know from last week that, at least based purely on its definition,\n",
    "P-value cannot be interpreted as probability of an association being real.\n",
    "Let's look what information and assumptions\n",
    "we would need in addition to the P-value in order to talk about\n",
    "probability of a non-zero effect.\n",
    "A review on Bayesian methods in GWAS by [Stephens & Balding](https://www.nature.com/articles/nrg2615)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c05d115",
   "metadata": {},
   "source": [
    "### From significance to the probability of association\n",
    "\n",
    "In order to talk about probability of a hypothesis, we need to define the\n",
    "set of all possible hypotheses whose combined probability is 1, i.e.,\n",
    "we work under the assumption that one of the hypotheses is true.\n",
    "\n",
    "(For P-value calculation we only ever define\n",
    "the null hypothesis and therefore we are unable to talk about the probability\n",
    "of the null hypothesis since we haven't defined what else is possible:\n",
    "Even if the data seem unlikely under the null hypothesis,\n",
    "if the data are even more unlikely under other possible hypotheses, then\n",
    "the null hypothesis might still be quite probable, and such\n",
    "considerations cannot be done from the P-value alone.)\n",
    "\n",
    "In our case, we consider only two hypotheses: $H_0: \\beta =0$ and $H_1: \\beta \\neq 0.$\n",
    "Next we need to quantify the **prior probabilities** of these hypotheses.\n",
    "These answer to the question: What would I consider as probability of each\n",
    "hypothesis before I have seen the data.\n",
    "The phrase \"What would _I_\" is there on purpose: prior probabilities are subjective. They are\n",
    "based on whatever knowledge _I_ have available. Therefore different persons may have different\n",
    "prior probabilities for the same hypothesis and my prior can (and will!)\n",
    "change as I learn more about the question.\n",
    "For example,\n",
    "$P(H_1) = 10^{-5}$ could be a reasonable prior for a non-zero effect based on\n",
    "what I know about GWAS.\n",
    "(Last week we saw how a magnitude more stringent assumption $P(H_1) = 10^{-6}$\n",
    "led us to the common GWS threshold.)\n",
    "\n",
    "Then we observe the data $\\mathcal{D}$ and our interest is in the probabilites\n",
    "of each hypothesis after we have seen the data. This is the core question of **Bayesian\n",
    "inference**: How does observing the data update our beliefs from our current state of knowledge,\n",
    "described by prior probabilities $P(H_i)$,\n",
    "into our **posterior probabilities** $P(H_i|\\mathcal{D})$?\n",
    "In short: How do we learn from data?\n",
    "Not surprisingly, the answer is the Bayes rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1028d",
   "metadata": {},
   "source": [
    "#### 4.1 Bayes rule\n",
    "To write down the Bayes rule (also called Bayes theorem, Bayes formula),\n",
    "we just remember that we are considering joint distributions of two variables.\n",
    "Here the two variables are the hypothesis $H_i$ and the observed data $\\mathcal{D}$.\n",
    "We expand their joint probability $P(H_i,\\mathcal{D})$ using conditional probability rule\n",
    "in both ways possible\n",
    "$$P(H_i | \\mathcal{D}) P(\\mathcal{D})= P(H_i,\\mathcal{D}) = P(\\mathcal{D} | H_i) P(H_i),$$\n",
    "from which we can solve the conditional probability that the hypothesis\n",
    "holds given that we have observed the data $\\mathcal{D}$:\n",
    "$$\n",
    "P(H_i|\\mathcal{D}) = \\frac{P(\\mathcal{D}|H_i)P(H_i)}{P(\\mathcal{D})},\\qquad\\textrm{ for  } i=0,1.\n",
    "$$\n",
    "This is the Bayes rule. The conditional probability on the\n",
    "left hand side is also called the **posterior probability**\n",
    "of the hypothesis given the data.\n",
    "\n",
    "Since the term $P(\\mathcal{D})$ (the marginal probability of the observed data) does\n",
    "not depend on hypothesis $H_i$, we can get rid of it by taking the ratio of the posteriors of\n",
    "the two hypotheses:\n",
    "$$\n",
    "\\frac{P(H_1|\\mathcal{D})}{P(H_0|\\mathcal{D})} = \\frac{P(\\mathcal{D}|H_1)P(H_1)}{P(\\mathcal{D}|H_0)P(H_0)}.\n",
    "$$\n",
    "Hence, in order to compute the posterior probability ratio for the hypotheses,\n",
    "we will still need the terms $P(\\mathcal{D}|H_i)$ in addition to the prior probabilities.\n",
    "$P(\\mathcal{D}|H_i)$ describes what kind of data sets\n",
    "we are likely to see under each hypothesis and with which probability.\n",
    "After these probability densities are specified, the inference\n",
    "is about letting the possible hypotheses compete both in how well they explain the\n",
    "observed data (terms $P(\\mathcal{D}|H_1)$ and $P(\\mathcal{D}|H_0)$)\n",
    "and in how probable they are *a priori* (prior probablity terms $P(H_1)$ and $P(H_0)$).\n",
    "\n",
    "**Example 4.1.** The Bayesian inference shows that both the observed data AND the\n",
    "prior knowledge is crucial for a complete inference.\n",
    "Suppose, for example, that a sequencing effort of my genome returns data\n",
    "$\\mathcal{D}$ that seems completely missing chromosome 6.\n",
    "We have two hypotheses: $H_0$: \"There is a technical error\",\n",
    "or $H_1$: \"I don't carry any copies of chromosome 6 (in the cells involved)\".\n",
    "The observed result could have\n",
    "resulted from either of these options\n",
    "and hence under both hypotheses $P(\\mathcal{D}|H_i)$ is similarly very high.\n",
    "So both hypotheses are consistent with the observations and P-values computed under\n",
    "either of the hypothesis as the null hypothesis would not show inconsistencies between\n",
    "observed data and the hypothesis.\n",
    "However, the prior odds of $P(H_1)/P(H_0)$ is pretty small\n",
    "if we think that it is more likely that there is a tecnical error\n",
    "than that I would be missing chr 6 (and still be pretty healthy).\n",
    "Hence the posterior conclusion that combines the prior probabilities\n",
    "and the observed data is that it is more likely that\n",
    "we have an error somewhere in the process than that I don't carry chr 6,\n",
    "even though the observation alone couldn't tell the two hypotheses apart.\n",
    "\n",
    "**Example 4.2.** Interpretation of a medical test result is [the standard\n",
    "example](https://en.wikipedia.org/wiki/Bayes%27_theorem#Drug_testing) of the use of Bayes rule.\n",
    "Let's apply it to a case where we try to determine whether an individual has a disease given his genotype.\n",
    "\n",
    "Suppose that each copy of *HLA-DRB1\\*1501* allele on chromosome 6 increases the risk of *multiple sclerosis*\n",
    "by OR=3. Prevalence of MS-diseases is $K=0.001$ and population frequency of DRB1\\*1501 is 0.20.\n",
    "What is probability of ever getting the disease for each genotype (i.e. 0,1 or 2 copies of DRB\\*1501)?\n",
    "\n",
    "**Answer.** Denote by $D$ the event of getting the disease and by $X$ the genotype.\n",
    "Here $D$ has the role of a hypothesis and $X$ the role of observed data in the above formulation of Bayes rule.\n",
    "Bayes rule says that for each genotype $x \\in \\{0,1,2\\}$:\n",
    "$$P(D\\,|\\, X = x) = \\frac{P(D)P(X=x\\, |\\, D)}{P(X=x)}.$$\n",
    "We know that $P(D)=K=0.001$ and we can assume that the control frequencies\n",
    "are approximately the population frequencies since the diseases has so low prevalence.\n",
    "Assuming HWE, the population frequencies are\n",
    "$P(X=0)=0.8^2=0.64$, $P(X=1)=2\\cdot 0.8 \\cdot 0.2 = 0.32$ and\n",
    "$P(X=2)=0.2^2 = 0.04.$\n",
    "It follows that case frequencies are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cc5c2ac",
   "metadata": {
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "K = 0.001\n",
    "or = 3\n",
    "q = c(0.64, 0.32, 0.04) #controls are like population for a low prevalence disease\n",
    "a = c(1, q[2]/q[1]*or, q[3]/q[1]*or^2) #see GWAS1 for how to get case freqs from controls and OR\n",
    "f0 = 1/sum(a) #P(X=0 | D)\n",
    "f = f0*a #case frequencies\n",
    "rbind(cases = f, controls = q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79302629",
   "metadata": {},
   "source": [
    "The risk to get the disease given the genotype is, according to Bayes rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04665fe",
   "metadata": {
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "rbind(genotype = c(0, 1, 2), risk = K*f/q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b537e51",
   "metadata": {},
   "source": [
    "So even though the disease status has a large effect on the genotype distributions,\n",
    "still even the high risk group has risk < 0.5% and the genotype doesn't give\n",
    "a practically useful predictive accuracy at the level of an individual from the population because\n",
    "the disease is so rare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fcfd34",
   "metadata": {},
   "source": [
    "#### 4.2 Probability model for observed GWAS data\n",
    "To use Bayes rule in GWAS setting, we need to define probability density for observed data\n",
    "under both the null hypothesis and the alternative hypothesis.\n",
    "\n",
    "In a linear regression GWAS model\n",
    "$y = \\mu + x \\beta + \\varepsilon$,\n",
    "the observed data $\\mathcal{D}_k=(\\pmb{y},\\pmb{x}_k)$ consist of the phenotype vector\n",
    "$\\pmb{y}$ and the vector of genotypes $\\pmb{x}_k$ at the tested variant $k$.\n",
    "When we assume Gaussian errors\n",
    "(i.e. each $\\varepsilon_i$ having a Normal distribution with same variance $\\sigma^2$)\n",
    "the probability density of data for a fixed value of effect size $\\beta$ and error variance\n",
    "$\\sigma^2$ is $$p\\left(\\mathcal{D}_k\\, |\\, \\beta, \\sigma^2 \\right) =\n",
    "\\mathcal{N}(\\pmb{y};\\,\\pmb{x}_k\\beta,\\,\\sigma^2 I) \\propto\n",
    "\\exp\\left(-(\\pmb{y}-\\pmb{x}_k\\beta)^T(\\pmb{y}-\\pmb{x}_k\\beta)/(2\\sigma^2)\\right).\n",
    "$$\n",
    "(We have ignored $\\mu$ here by assuming that $y$ and $x_k$ are mean-centered values; this\n",
    "just simplifies the notation but doesn't affect the results.)\n",
    "\n",
    "Under the null model, we set $\\beta=0$ and\n",
    "in the alternative model,\n",
    "we can set $\\beta$ to some other value $b_1$. If we do not want to specify\n",
    "our model of true effects by a single value $b_1$, we can use a **prior\n",
    "distribution** for $\\beta$, for example, by saying that under the alternative model\n",
    "$\\beta \\sim \\mathcal{N}(b_1,\\tau_1^2)$.\n",
    "This means that if the alternative model holds, then the true effect sizes\n",
    "are distributed around value $b_1$ with sd of $\\tau_1$.\n",
    "With this prior distribution, the probability of data under $H_1$\n",
    "is given by weighting the above likelihood function by the\n",
    "prior probability of each possible\n",
    "value of $\\beta$: $$p(\\mathcal{D}_k\\,|\\,H_1) = \\int_\\beta\n",
    "p\\left(\\mathcal{D}_k\\,|\\,\\beta,\\sigma^2\\right) p(\\beta\\,|\\,H_1) d\\beta = \\int_\\beta\n",
    "\\mathcal{N}\\left(\\pmb{y};\\,\\pmb{x}_k\\beta,\\,\\sigma^2\\right)\n",
    "\\mathcal{N}\\left(\\beta;\\, b_1,\\, \\tau_1^2\\right) d\\beta.$$ (In both models we\n",
    "typically fix $\\sigma^2$ to its empirical maximum likelihood estimate as the\n",
    "competing regression models do not typically differ in their prior on\n",
    "$\\sigma^2$, and hence we are less interested in it than in $\\beta$.)\n",
    "\n",
    "If we assume, that in the Gaussian prior of $\\beta$, the mean parameter $b_1=0$,\n",
    "then the integral can be done analytically to give $$P(\\mathcal{D}_k\\,|\\,H_1) =\n",
    "c\\cdot \\mathcal{N}\\left(\\widehat{\\beta};\\, 0, \\, \\tau_1^2 + \\textrm{SE}^2\n",
    "\\right),$$ where $c$ is a constant and $\\widehat{\\beta}$ is the MLE of $\\beta$\n",
    "and SE the corresponding standard error. Note that by replacing $\\tau_1$ with 0,\n",
    "we have $$P(\\mathcal{D}_k\\,|\\,H_0) = c\\cdot \\mathcal{N}\\left(\\widehat{\\beta};\\, 0, \\,\n",
    "\\textrm{SE}^2  \\right).$$ These results tell that we can quantify how well each\n",
    "model explains the data, by asking how well each model can explain the MLE\n",
    "$\\widehat{\\beta}$. Let's demonstrate this.\n",
    "\n",
    "**Example 4.3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a84127",
   "metadata": {
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "n = 3000 #sample size for SE calculation\n",
    "f = 0.3 #MAF for SE calculation\n",
    "sigma = 1 #error SD\n",
    "se = sigma / sqrt(2*f*(1-f)*n) #SE for QT GWAS\n",
    "tau = 0.1 #prior standard deviation for effect size beta under H1\n",
    "\n",
    "#Let's draw probability densities of \"data\" under the two models, H0 and H1\n",
    "#as a function of MLE estimate\n",
    "x = seq(-0.3, 0.3, by = 0.01)\n",
    "y1 = dnorm(x, 0, sqrt(tau^2 + se^2) )\n",
    "y0 = dnorm(x, 0, se)\n",
    "plot(x, y0, t = \"l\", col = \"cyan\", lwd = 2, xlab = \"estimate of beta\",\n",
    "     ylab = \"probability density of data\")\n",
    "lines(x, y1, col = \"magenta\", lwd = 2)\n",
    "legend(\"topright\", c(\"H0\",\"H1\"), col=c(\"cyan\",\"magenta\"), lwd = 2)\n",
    "\n",
    "#We make a shortcut and don't simulate data at all, but we simulate estimates\n",
    "#Suppose we have two cases, first is null, second is alternative (true betas are 0 and 0.2)\n",
    "b =c(0, 0.2)\n",
    "b.est = rnorm(2, b, se) #these are simulated estimates: true means and Gaussian noise determined by SE\n",
    "points(b.est, c(0,0), pch = 19, col = c(\"blue\",\"red\") )\n",
    "#Next: log of Bayes factor of H1 vs H0, explained below\n",
    "#       use log-scale to avoid inacuracies.\n",
    "log.bf.10 = dnorm(b.est, 0, sqrt(tau^2 + se^2), log=T ) - dnorm(b.est, 0, se, log = T)\n",
    "bf.10 = exp(log.bf.10) #then turn from log-scale to Bayes factor scale\n",
    "text(b.est[1], 4, signif(bf.10[1], 2), col = \"blue\")\n",
    "text(b.est[2], 4, signif(bf.10[2], 2), col = \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7053e147",
   "metadata": {},
   "source": [
    "The distribution of $H_0$ desribes what kind of effect estimates we can get with this\n",
    "sample size and MAF when true effect is exactly 0. Any deviation from 0 is then by\n",
    "statistical sampling effect (as quantified by SE).\n",
    "\n",
    "The distribution of $H_1$ describes what kind of effect estimates we expect when we have\n",
    "BOTH a true non-zero effect (whose expected range is described by standard deviation of $\\tau_1$)\n",
    "and we have ALSO a statistical sampling effect (as quantified by SE).\n",
    "\n",
    "If observed estimate $\\widehat{\\beta}$ is close to 0, then $H_0$ explains the data better than\n",
    "$H_1$, whereas the opposite is true when $\\widehat{\\beta}$ is farther away from\n",
    "0. With these parameters, $H_1$ starts to dominate about when $|\\widehat{\\beta}|\n",
    "\\geq  0.05$. Values close to zero are relatively more likely under the null\n",
    "than under the alternative because the alternative model can also explain observations\n",
    "that are farther away from the zero and hence its probability mass is less concentrated around 0\n",
    "than that of the null which can only explain well data sets with effect estimates near zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a24625e",
   "metadata": {},
   "source": [
    "#### 4.3 Bayes factor\n",
    "\n",
    "Two points shown in the plot above are examples of possible estimates that could result\n",
    "either under $H_0$ (blue) or $H_1$ (red). The values are ratios of\n",
    "$P(\\mathcal{D}|H_1)/P(\\mathcal{D}|H_0)$ computed at these two points. When this\n",
    "ratio $<1$ the null model $H_0$ explains the data better and when it is $>1$\n",
    "the opposite is true. This ratio is called **Bayes factor (BF)** and it is the\n",
    "factor that multiplies the prior odds to get to the posterior odds: $$\n",
    "\\underbrace{\\frac{P(H_1|\\mathcal{D})}{P(H_0|\\mathcal{D})}}_{\\textrm{posterior odds}} =\n",
    "\\underbrace{\\frac{P(\\mathcal{D}|H_1)}{P(\\mathcal{D}|H_0)}}_{\\textrm{Bayes factor}} \\times\n",
    "\\underbrace{\\frac{P(H_1)}{P(H_0)}}_{\\textrm{prior odds}}. $$\n",
    "To interpret the Bayes factor, we can think that in order that the\n",
    "posterior probability of the alternative model would be at least 10 times higher\n",
    "than that of the null, the BF needs to be at least 10 times higher than\n",
    "the inverse of the prior odds.\n",
    "If prior odds are 1e-5, then a BF of 1e+6 would give posterior odds > 10.\n",
    "\n",
    "We are almost there having calculated a proper probability for the null hypothesis.\n",
    "We still need to agree on the prior probability of the null model.\n",
    "Last week we saw that current genomewide-significant level can be thought to\n",
    "correspond to a prior probability of about $P(H_1)=10^{-6}$.\n",
    "We know that this seems very stringent compared to\n",
    "true architecture behind complex traits and diseases,\n",
    "but it gives us a conservative reference point.\n",
    "With this prior probability,\n",
    "the posterior odds and posterior probabilities for the alternative model are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb06cadd",
   "metadata": {
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "post.odds = bf.10*1e-6/(1-1e-6) #P(H_1|D) / P(H_0|D)\n",
    "post.prob = post.odds/(1+post.odds) #P(H_1|D)\n",
    "paste(post.prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41240263",
   "metadata": {},
   "source": [
    "For illustration, let's check the P-values corresponding to these two data sets\n",
    "using a Wald statistic $\\widehat{\\beta}/\\textrm{SE}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c132805c",
   "metadata": {
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "pchisq( (b.est/se)^2, df = 1, lower = F )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b4c18",
   "metadata": {},
   "source": [
    "So P-value of the first one is quite close to 0.2, whereas the second is\n",
    "way beyond 5e-8. And the Bayesian analysis said that\n",
    "the first one is almost certain to be null whereas the second one is\n",
    "almost certain to have a non-zero effect.\n",
    "Thus, there is no difference in the conclusion of the standard P-value based GWAS inference\n",
    "and the Bayesian inference, which is typically the case when there is enough data.\n",
    "Conceptually, however, there is a large difference between the P-value,\n",
    "which is probability of at least as extreme data under the null, and the posterior\n",
    "probability of the hypothesis itself.\n",
    "\n",
    "There were several assumptions made in the Bayesian anaysis about the\n",
    "effect sizes under $H_1$ and also on the prior probabilities of the models, and\n",
    "the posterior probabilities will change when these assumptions are changed.\n",
    "Therefore, P-values remain useful simple summaries of data that can\n",
    "be computed easily. The important thing\n",
    "is to know what P-values are and what they are not, and that what\n",
    "kind of additional pieces of information would be needed in order to\n",
    "appropriately quantify the probabilities of the hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f92c1ea",
   "metadata": {},
   "source": [
    "#### 4.4 Approximate Bayes factor in GWAS\n",
    "\n",
    "The calculation of the Bayes factor above, that was\n",
    "based on the maximum likelihood estimate $\\widehat{\\beta}$\n",
    "and its SE, was proposed by\n",
    "[Jon Wakefield in 2009](https://onlinelibrary.wiley.com/doi/abs/10.1002/gepi.20359). The formula is\n",
    "$$\n",
    "\\textrm{ABF} \\approx \\frac{P(\\mathcal{D}|H_1)}{P(\\mathcal{D}|H_0)}\n",
    "\\approx \\frac{\\mathcal{N}\\left(\\widehat{\\beta};\\, 0, \\, \\tau_1^2 + \\textrm{SE}^2\n",
    "\\right)}{\\mathcal{N}\\left(\\widehat{\\beta};\\, 0, \\, \\textrm{SE}^2\n",
    "\\right)} = \\frac{(2\\pi)^{-0.5}(\\tau_1^2 + \\textrm{SE}^2)^{-0.5}\\exp\\left(-\\frac{1}{2}\\frac{\\widehat{\\beta}^2}{\\tau_1^2 + \\textrm{SE}^2}\\right) }\n",
    "{(2\\pi)^{-0.5}(\\textrm{SE}^2)^{-0.5}\\exp\\left(-\\frac{1}{2}\\frac{\\widehat{\\beta}^2}{ \\textrm{SE}^2}\\right) }\n",
    "$$\n",
    "$$\n",
    "=\\sqrt{\\frac{\\textrm{SE}^2}{\\tau_1^2 + \\textrm{SE}^2}}\n",
    "\\exp\\left(\\frac{1}{2} \\frac{\\widehat{\\beta}^2}{\\textrm{SE}^2} \\frac{\\tau_1^2}{\\tau_1^2 + \\textrm{SE}^2} \\right)\n",
    ",\n",
    "$$\n",
    "where the alternative model is specified by effect size prior\n",
    "$H_1: \\beta \\sim \\mathcal{N}(0,\\tau_1^2).$ We have presented the ABF in the form\n",
    "where the alternative is in the numerator and null in the denominator.\n",
    "Hence large ABF means strong evidence in favor of the alternative model.\n",
    "(Wakefield's paper used the inverse of this quantity as Bayes factor, i.e.,\n",
    "it computes Bayes factor comparing null to the alternative whereas\n",
    "we compare alternative to null.)\n",
    "\n",
    "In R, this is easy to compute using `dnorm()` function, as we did above.\n",
    "It is always good to do the ratio of densities on log-scale to avoid possible numerical\n",
    "underflows/overflows. In `dnorm()` this happens by adding `log = TRUE` to the command.\n",
    "Then the ratio of densities becomes a difference between log-densities:\n",
    "```\n",
    "log.bf = dnorm(b.est, 0, sqrt(tau^2 + se^2), log = T ) - dnorm(b.est, 0, se, log = T)\n",
    "bf = exp(log.bf) #turn from log-scale to Bayes factor scale\n",
    "```\n",
    "\n",
    "The same formula can be used when $\\widehat{\\beta}$ and its SE originate\n",
    "from a disease study analyzed by logistic regression. In that case, the formula is\n",
    "an approximation based on the assumption that the logistic regression likelihood has\n",
    "a shape of a Gaussian density function. Therefore, this approach is generally called\n",
    "**Approximate Bayes Factor (ABF)**.\n",
    "\n",
    "ABF can be computed from the observed GWAS data ($\\widehat{\\beta},\\textrm{SE}$)\n",
    "once we have chosen the variance $\\tau_1^2$\n",
    "of the effect size distribution under the alternative. How should we do that?\n",
    "\n",
    "**Example 4.4.** Let's assume that the non-zero effects have a distribution\n",
    "$\\mathcal{N}(0,\\tau_1^2)$ and we want to determine $\\tau_1$ in such a way that\n",
    "with 95% probability the effect (of a SNP with MAF = 0.25)\n",
    "explains less than proportion $p$ of\n",
    "the phenotypic variance $v = \\textrm{Var}(y)$.\n",
    "We will first compute the effect size $\\beta_p$ that explains exactly phenotypic\n",
    "variance of $p\\cdot v$, and then we will find the sd parameter $\\tau_1$\n",
    "for which 95% of the probability mass of $\\mathcal{N}(0,\\tau_1^2)$ is within the\n",
    "region $(-\\beta_p, \\beta_p)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2308ff04",
   "metadata": {
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "v = 1 #Set this to the phenotypic variance\n",
    "p = 0.01 #effect explains less than 1% of the trait variance,\n",
    "target.prob = 0.95 #with this probability\n",
    "maf = 0.25\n",
    "# 2*maf*(1-maf)*b^2 = p*v --> b = +/- sqrt(p*v/(2*maf*(1-maf)))\n",
    "b = sqrt(p*v / ( 2*maf*(1-maf) ) )\n",
    "tau.seq = seq(0, 1, 0.001) #grid to evaluate tau\n",
    "x = pnorm(b, 0, tau.seq, lower = F) #what is the upper tail prob. at b for each value of tau?\n",
    "tau.1 = tau.seq[which.min( abs(x - (1 - target.prob)/2) )] #which is closest to target prob?\n",
    "#Check that the probability mass in (-b,b) is indeed close to target\n",
    "print(paste0(\"tau.1=\",tau.1,\" has mass \",signif(1 - 2*pnorm(b, 0, tau.1, lower = F),3),\n",
    "             \" in (-\",signif(b,4),\", \",signif(b,4),\").\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2c0ca3",
   "metadata": {},
   "source": [
    "**Example 4.5.** For case-control GWAS, we want to find such $\\tau_1$ that with 95%\n",
    "probability a variant can increase risk by at most OR of 1.30.\n",
    "Now we get the critical point $\\beta_p$ directly as $\\log(1.30)$ and then proceed\n",
    "as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6902338",
   "metadata": {
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "or = 1.30 #effect is at most this large\n",
    "target.prob = 0.95 # with this probability\n",
    "b = log(or)\n",
    "tau.seq = seq(0, 1, 0.001) #grid to evaluate tau\n",
    "x = pnorm(b, 0, tau.seq, lower = F) #what is the upper tail prob. at b for each value of tau?\n",
    "tau.1 = tau.seq[which.min( abs(x - (1 - target.prob)/2) )] #which is closest to target prob?\n",
    "#Check that the probability mass in (-b,b) is indeed close to target\n",
    "print(paste0(\"tau.1=\",tau.1,\" has mass \",signif(1 - 2*pnorm(b, 0, tau.1, lower = F),3),\n",
    "             \" in (-\",signif(b,4),\", \",signif(b,4),\").\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a278d9",
   "metadata": {},
   "source": [
    "Note that the above choices of $\\tau_1$ do not model very large effect sizes.\n",
    "There are variants that explain, say, over 10% of the variation of a quantitative trait\n",
    "or that have an OR of 3 for some disease. To properly model them in the Bayesian framework,\n",
    "one would need to use several prior distributions and average the results\n",
    "(Bayesian model averaging)."
   ]
  }
 ],
 "metadata": {
  "Rmd_chunk_options": {
   "author": "Matti Pirinen, University of Helsinki",
   "date": "Updated: 29-10-2020; 1st ver: 23-Jan-2019.",
   "output": {
    "html_document": "default"
   },
   "title": "GWAS 4: Bayesian inference",
   "urlcolor": "blue"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ],
    [
     "R",
     "ir",
     "",
     ""
    ]
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
